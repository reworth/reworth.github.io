<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[game data analysis]]></title>
    <url>%2F2018%2F01%2F31%2Fgame-data-analysis%2F</url>
    <content type="text"><![CDATA[数据分析为运营的必备技能之一，无论你是研发商，渠道商甚至发行商，都用得上。这篇就简单的介绍一下各种指数并且给出分析思路，用的统计工具为常见的友盟，本文中数据皆为虚拟数据。 先来看应用总览： 累计用户这个大家都明白，为游戏上线以来的所有用户统计。其中重点关注下面几个数据： 付费率（重要指标）：付费玩家所占据总玩家的比例，如果细分的话，付费率可以分为注册用户付费率，平均在线付费率和活跃用户付费率。图片中为5.93%，在这里的意思是100个活跃玩家有接近6个付费。 大家玩过一些页游，手游，就可以看到 各种金光闪闪的《首冲大礼》，做过研发的同学估计深有体会，很多都会单独做个ICON来提醒玩家：首冲只要1元，即可获得价值188元的奖励！为的是什么，就是提高这个指数：付费率。你可以用户少，你可以人均付费少，但是付费率高，一样可以得到渠道青睐，获得更多推荐展示机会（行业俗称为“吸量”）。 ARPU（重要指标）：平均每用户收入。可以理解为收入除以用户数。图片中实例为26，相当于每个玩家人均付费26元。 ARPPU（重要指标）：平均每付费用户收入。可以理解为收入除以付费用户数。 这三个指数理论上当然是越高越好，我们通过这些数据对游戏进行付费优化，比如某游戏人均付费率很低，但是ARPPU很高，那就说明大R的付费能力强，针对这一点，我们做一些付费功能调整和优化，甚至专属大R客服，让大R玩的更开心。 再比如某游戏人均付费率高，但是ARPPU很低，说明大部分玩家喜欢花一些小钱，那么我们针对这部分玩家，多增加一些小额的礼包。这里就牵扯到 “用户画像” 这个概念，稍后我们结合其他数据一起来说明。 新增用户分析 说到新增用户，那就离不开投放和转化率这些概念，我们知道，一个APP或者游戏上架，用户会点击下载，那么这个用户是哪里来的呢？简单来分为2个方面。 自然用户：俗称自然量，没做推广，没做广告，用户自发下载的。 好的运营，会不花钱吸引用户，提升展示机会，比如关键字优化啊，蹭热度啊（根据当前热点事件写软文植入），游戏包体优化(googleplay限制包体100M以内)，展示ICON优化等。 推广用户：投放广告或者宣传活动，用户看到，接收到信息，吸引而来。 这里的数据是可以结合投放来一起分析，比如策划了一个推广活动，花费20万，吸引了10万用户，这次活动新增用户成本为2元；第二次举办了一个活动，花费20万，只来了5万用户，不考虑用户成本递增的话，就要开始分析问题了： 1.这次活动投放目标群体是否有问题？是否精准？ 2.这次活动策划方案是否起到作用？是否有爆点？ 3.这次活动是否执行的很好？有无遗漏？ 4.有哪些教训吸取和避免？ 同理，如果活动做的很成功，同样有思考的部分，哪些地方做得好，可以当做经验和再次投放侧重方向。 留存率 次日留存率（重要指标）：（当天新增的用户中，在注册的第2天还登录的用户数）/第一天新增总用户数 7日留存率（重要指标）：（第一天新增的用户中，在注册的第7天还有登录的用户数）/第一天新增总用户数 这两项是渠道和研发商都非常重视的数据，一般项目上线都有个测试期，俗称“调数据”，主要就是优化次留，7留情况。 所以现在游戏大部分都有7天连续登陆大礼包，第七天送紫色卡牌，紫色装备，有量的话语方（渠道）非常看重，为了数据好看，大家使出浑身招式留住玩家。 根据留存来分析问题： 1.次留率很低，大部分玩家第二天不再上线，可能引起的原因：新手阶段不友好；开场不吸引人；游戏上手难度大；功能引导太繁琐；程序bug太多，闪退，卡死，无法登陆等。 2.次留率不低，但是第3-4天大量流失，可能引起的原因：游戏内容重复，单调；游戏挫败感太强；新手无对应保护等。 版本数据 游戏每个版本还有多少人在玩，这个数据其实对游戏相对不太重要，大家了解下热更新，整包更新，版本兼容即可。 对于APP而言，更新大版本的时候，总有一些用户怀念老版本的界面风格等，可以依次作为参考。 渠道用户数据 我们知道，iOS系统渠道较少，主要就是App Store和早期的一些越狱渠道比如91、PP助手、同步推、Itools、快用苹果助手等。 现在很多上游戏的顺序是先越狱渠道，作为测试，再App Store，最后安卓。因为苹果机型相对少，适配简单，但App Store审核时间较长，而游戏都是要不断修改优化的，故先上越狱渠道来测试，改完了再上App Store。用户虽然越狱了，但依然是iOS用户，ARPU值相对会比安卓高一些。 而安卓这边就混乱一些，一来机型繁多适配测试时间长，二来渠道太多依次接入各家SDK，我曾经参与的一款单机游戏，一次就要打出30多个安卓渠道包。 根据渠道用户数量分析，就可以针对推出一些活动，比如渠道专用的激活码，大礼包等。哪些渠道用户数量多，投入更多时间，主推这个渠道；哪些渠道相对用户数少，维护可以少一些。集中力量做重要的渠道。 终端设备数据 万万没想到….我们设备第一名竟然是李易峰杨幂同款….根据用户机型，做对应的优化，适配，测试。 网络及运营商数据 大部分的用户，起码95%用的都是wifi，说明我国4G还是比较贵啊…哈哈。根据这个信息，游戏可以相对更新一些比较大的数据包。 至于运营商，妥妥的移动第一名，甩来联通电信一大截。 用户地域 可以看到各个省份的玩家数量，所在国家。 关于地域，其实也有很多内容可以思考，哪些区域用户特征比较明显，比如四川麻将等相对地域性的玩法，在本地的接受程度就比较高一些。比如某些国家有一些忌讳等要避开。 其实这些都是在进行用户画像，通过大数据来推断出我们的用户是谁，看到这里，大家心里应该已经勾勒出用户的大概情况： 买得起3000元左右的新款手机，有一些消费能力，玩游戏的时候大部分都有WiFi（室内），非驴友，大部分玩游戏时间在周末…..别着急，我们接着看其他数据。 付费用户趋势 蓝色为新付费用户：当日首次付费的用户。橙色为老付费用户。如图可以看到我们新增付费用户比较高，说明我们对新用户的付费吸引更大，那么需要做下面这些思考。 1.老用户的付费提升：如何留住老用户，并让他们付费；相对新用户，老用户更需要的是什么；提高老用户付费冲动，比如推出更牛逼后期高等级穿的装备，作为目标让老用户追求；游戏版本是否需要更新，增加新内容给老用户。 2.新用户付费优化：大部分充值的玩家会多次充值，如何留住这些已经充值的用户，哪些功能细节做得好，可以借鉴。 使用时长 根据数据，我们用户单次玩游戏大部分集中在3-10分钟这个区间段。 使用时长这个是有因果关系的，在项目初期，我们就会给项目定位，是轻度的休闲游戏还是偏中度重度的MMO等。假如我们定位于轻度休闲类，一局几分钟，玩家每天玩个几把，那么项目的生命周期，各个系统包括付费都会围绕这个展开，上线后看数据，跟我们预想一样，那就稳了，如果到时候一看数据，玩家每天投入2-3小时，那就得思考是哪里出的问题导致偏差，如何调整补救等。 典型的例子比如COC，皇室战争，每局几分钟，COC中的建筑升级需要时间，造兵需要时间，英雄的恢复需要时间，圣水收集需要时间，设计的循环是：上线准备——一局战斗——下线——恢复补兵——上线准备——一局战斗。 启动次数 每天玩家启动的次数统计，当然我们可以设计一些内容来提高用户启动次数。 比如每隔2小时开一个宝箱，玩家就会每隔几个小时上线看看。 自定义事件 我们可以通过定义一些事件来获取数据，帮助我们更好分析，所谓的“精细化运营”。 比如我们可以定义一个“玩家查看礼包次数”和“玩家购买礼包次数”，根据数据我们进行思考：为什么这2个次数差距大，是礼包不吸引人？玩家不喜欢这个礼包？玩家很喜欢但是嫌贵？礼包内道具配置不合理？ 还可以根据“某关卡进入次数”和“某关卡死亡次数”来分析关卡的难易度，是否需要调整等。 好了，今天先说到这里，我饿了去吃东西了。祝大家新年越来越好。]]></content>
      <tags>
        <tag>game data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[threading-lock]]></title>
    <url>%2F2018%2F01%2F26%2Fthreading-lock%2F</url>
    <content type="text"><![CDATA[不使用lock的情况函数一：全局变量A的值每次加1，循环10次，并打印 12345def job1(): global A for i in range(10): A+=1 print(&apos;job1&apos;,A) 函数二：全局变量A的值每次加10，循环10次，并打印 12345def job2(): global A for i in range(10): A+=10 print(&apos;job2&apos;,A) 主函数：定义两个线程，分别执行函数一和函数二 12345678if __name__== &apos;__main__&apos;: A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 完整代码： 1234567891011121314151617181920212223import threadingdef job1(): global A for i in range(10): A+=1 print(&apos;job1&apos;,A)def job2(): global A for i in range(10): A+=10 print(&apos;job2&apos;,A)if __name__== &apos;__main__&apos;: lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 运行结果（在spyder编译器下运行的打印结果）： 1234567891011121314151617181920job1job2 11job2 21job2 31job2 41job2 51job2 61job2 71job2 81job2 91job2 101 1job1 102job1 103job1 104job1 105job1 106job1 107job1 108job1 109job1 110 可以看出，打印的结果非常混乱 使用 Lock 的情况lock在不同线程使用同一共享内存时，能够确保线程之间互不影响，使用lock的方法是， 在每个线程执行运算修改共享内存之前，执行lock.acquire()将共享内存上锁， 确保当前线程执行时，内存不会被其他线程访问，执行运算完毕后，使用lock.release()将锁打开， 保证其他的线程可以使用该共享内存。 函数一和函数二加锁 123456789101112131415def job1(): global A,lock lock.acquire() for i in range(10): A+=1 print(&apos;job1&apos;,A) lock.release()def job2(): global A,lock lock.acquire() for i in range(10): A+=10 print(&apos;job2&apos;,A) lock.release() 主函数中定义一个Lock 123456789if __name__== &apos;__main__&apos;: lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 完整的代码 123456789101112131415161718192021222324252627import threadingdef job1(): global A,lock lock.acquire() for i in range(10): A+=1 print(&apos;job1&apos;,A) lock.release()def job2(): global A,lock lock.acquire() for i in range(10): A+=10 print(&apos;job2&apos;,A) lock.release()if __name__== &apos;__main__&apos;: lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 运行结果 1234567891011121314151617181920job1 1job1 2job1 3job1 4job1 5job1 6job1 7job1 8job1 9job1 10job2 20job2 30job2 40job2 50job2 60job2 70job2 80job2 90job2 100job2 110 从打印结果来看，使用lock后，一个一个线程执行完。使用lock和不使用lock，最后打印输出的结果是不同的。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>threading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[路由协议]]></title>
    <url>%2F2018%2F01%2F19%2F%E8%B7%AF%E7%94%B1%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[路由控制的定义IP地址与路由控制互联网是由路由器连接的网络组合而成的，为了能让数据包正确达到目标主机，路由器必须在途中进行正确的转发。这种向“正确方向”装发数据所进行的处理就叫做路由控制或路由。路由器是根据路由控制表转发数据。 静态路由与动态路由静态路由是指事先设置好路由器和主机中将路由信息固定的一种方法。动态路由是指让路由协议在运行过程中自动地设置路由控制信息的一种方法。 路由控制范围人们根据路由控制的范围常使用IGP（Interior Gateway Protocol）和EGP（Exterior Gateway Protocol）两种类型的路由协议。 路由算法路由控制有各种各样的算法，其中最具代表性的两种，是距离向量（Distance-Vector）算法和链路状态（Link-State）算法。距离向量算法DV是根据距离和方向决定目标网络或目标主机位置的一种方法。链路状态算法是路由器在了解网络整体连接状态的基础上生成路由控制表的一种方法。 主要路由协议 路由协议名 下一层协议 方式 使用范围 循环检测 RIP UDP 距离向量 域内 不可以 RIP2 UDP 距离向量 域内 不可以 OSFP IP 链路状态 域内 可以 EGP IP 距离向量 对外连接 不可以 BGP TCP 距离向量 对外连接 不可以]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-Persistence]]></title>
    <url>%2F2018%2F01%2F17%2FRedis-Persistence%2F</url>
    <content type="text"><![CDATA[Redis PersistenceRedis provides a different range of persistence options: The RDB persistence performs point-in-time snapshots of your dataset at specified intervals. the AOF persistence logs every write operation received by the server, that will be played again at server startup, reconstructing the original dataset. Commands are logged using the same format as the Redis protocol itself, in an append-only fashion. Redis is able to rewrite the log on background when it gets too big. If you wish, you can disable persistence at all, if you want your data to just exist as long as the server is running. It is possible to combine both AOF and RDB in the same instance. Notice that, in this case, when Redis restarts the AOF file will be used to reconstruct the original dataset since it is guaranteed to be the most complete. The most important thing to understand is the different trade-offs between the RDB and AOF persistence. Let’s start with RDB: RDB advantages RDB is a very compact single-file point-in-time representation of your Redis data. RDB files are perfect for backups. For instance you may want to archive your RDB files every hour for the latest 24 hours, and to save an RDB snapshot every day for 30 days. This allows you to easily restore different versions of the data set in case of disasters. RDB is very good for disaster recovery, being a single compact file can be transferred to far data centers, or on Amazon S3 (possibly encrypted). RDB maximizes Redis performances since the only work the Redis parent process needs to do in order to persist is forking a child that will do all the rest. The parent instance will never perform disk I/O or alike. RDB allows faster restarts with big datasets compared to AOF. RDB disadvantages RDB is NOT good if you need to minimize the chance of data loss in case Redis stops working (for example after a power outage). You can configure different save points where an RDB is produced (for instance after at least five minutes and 100 writes against the data set, but you can have multiple save points). However you’ll usually create an RDB snapshot every five minutes or more, so in case of Redis stopping working without a correct shutdown for any reason you should be prepared to lose the latest minutes of data. RDB needs to fork() often in order to persist on disk using a child process. Fork() can be time consuming if the dataset is big, and may result in Redis to stop serving clients for some millisecond or even for one second if the dataset is very big and the CPU performance not great. AOF also needs to fork() but you can tune how often you want to rewrite your logs without any trade-off on durability. AOF advantages Using AOF Redis is much more durable: you can have different fsync policies: no fsync at all, fsync every second, fsync at every query. With the default policy of fsync every second write performances are still great (fsync is performed using a background thread and the main thread will try hard to perform writes when no fsync is in progress.) but you can only lose one second worth of writes. The AOF log is an append only log, so there are no seeks, nor corruption problems if there is a power outage. Even if the log ends with an half-written command for some reason (disk full or other reasons) the redis-check-aof tool is able to fix it easily. Redis is able to automatically rewrite the AOF in background when it gets too big. The rewrite is completely safe as while Redis continues appending to the old file, a completely new one is produced with the minimal set of operations needed to create the current data set, and once this second file is ready Redis switches the two and starts appending to the new one. AOF contains a log of all the operations one after the other in an easy to understand and parse format. You can even easily export an AOF file. For instance even if you flushed everything for an error using a FLUSHALL command, if no rewrite of the log was performed in the meantime you can still save your data set just stopping the server, removing the latest command, and restarting Redis again. AOF disadvantages AOF files are usually bigger than the equivalent RDB files for the same dataset. AOF can be slower than RDB depending on the exact fsync policy. In general with fsync set to every second performances are still very high, and with fsync disabled it should be exactly as fast as RDB even under high load. Still RDB is able to provide more guarantees about the maximum latency even in the case of an huge write load. In the past we experienced rare bugs in specific commands (for instance there was one involving blocking commands like BRPOPLPUSH) causing the AOF produced to not reproduce exactly the same dataset on reloading. This bugs are rare and we have tests in the test suite creating random complex datasets automatically and reloading them to check everything is ok, but this kind of bugs are almost impossible with RDB persistence. To make this point more clear: the Redis AOF works incrementally updating an existing state, like MySQL or MongoDB does, while the RDB snapshotting creates everything from scratch again and again, that is conceptually more robust. However - 1) It should be noted that every time the AOF is rewritten by Redis it is recreated from scratch starting from the actual data contained in the data set, making resistance to bugs stronger compared to an always appending AOF file (or one rewritten reading the old AOF instead of reading the data in memory). 2) We never had a single report from users about an AOF corruption that was detected in the real world. Ok, so what should I use?The general indication is that you should use both persistence methods if you want a degree of data safety comparable to what PostgreSQL can provide you. If you care a lot about your data, but still can live with a few minutes of data loss in case of disasters, you can simply use RDB alone. There are many users using AOF alone, but we discourage it since to have an RDB snapshot from time to time is a great idea for doing database backups, for faster restarts, and in the event of bugs in the AOF engine. Note: for all these reasons we’ll likely end up unifying AOF and RDB into a single persistence model in the future (long term plan). The following sections will illustrate a few more details about the two persistence models. SnapshottingBy default Redis saves snapshots of the dataset on disk, in a binary file called dump.rdb. You can configure Redis to have it save the dataset every N seconds if there are at least M changes in the dataset, or you can manually call the SAVE or BGSAVEcommands. For example, this configuration will make Redis automatically dump the dataset to disk every 60 seconds if at least 1000 keys changed: 1save 60 1000 This strategy is known as snapshotting. How it worksWhenever Redis needs to dump the dataset to disk, this is what happens: Redis forks. We now have a child and a parent process. The child starts to write the dataset to a temporary RDB file. When the child is done writing the new RDB file, it replaces the old one. This method allows Redis to benefit from copy-on-write semantics. Append-only fileSnapshotting is not very durable. If your computer running Redis stops, your power line fails, or you accidentally kill -9 your instance, the latest data written on Redis will get lost. While this may not be a big deal for some applications, there are use cases for full durability, and in these cases Redis was not a viable option. The append-only file is an alternative, fully-durable strategy for Redis. It became available in version 1.1. You can turn on the AOF in your configuration file: 1appendonly yes From now on, every time Redis receives a command that changes the dataset (e.g. SET) it will append it to the AOF. When you restart Redis it will re-play the AOF to rebuild the state. Log rewritingAs you can guess, the AOF gets bigger and bigger as write operations are performed. For example, if you are incrementing a counter 100 times, you’ll end up with a single key in your dataset containing the final value, but 100 entries in your AOF. 99 of those entries are not needed to rebuild the current state. So Redis supports an interesting feature: it is able to rebuild the AOF in the background without interrupting service to clients. Whenever you issue a BGREWRITEAOF Redis will write the shortest sequence of commands needed to rebuild the current dataset in memory. If you’re using the AOF with Redis 2.2 you’ll need to run BGREWRITEAOF from time to time. Redis 2.4 is able to trigger log rewriting automatically (see the 2.4 example configuration file for more information). How durable is the append only file?You can configure how many times Redis will fsync data on disk. There are three options: fsync every time a new command is appended to the AOF. Very very slow, very safe. fsync every second. Fast enough (in 2.4 likely to be as fast as snapshotting), and you can lose 1 second of data if there is a disaster. Never fsync, just put your data in the hands of the Operating System. The faster and less safe method. The suggested (and default) policy is to fsync every second. It is both very fast and pretty safe. The always policy is very slow in practice (although it was improved in Redis 2.0) – there is no way to make fsync faster than it is. What should I do if my AOF gets corrupted?It is possible that the server crashes while writing the AOF file (this still should never lead to inconsistencies), corrupting the file in a way that is no longer loadable by Redis. When this happens you can fix this problem using the following procedure: Make a backup copy of your AOF file. Fix the original file using the redis-check-aof tool that ships with Redis: $ redis-check-aof –fix Optionally use diff -u to check what is the difference between two files. Restart the server with the fixed file. How it worksLog rewriting uses the same copy-on-write trick already in use for snapshotting. This is how it works: Redis forks, so now we have a child and a parent process. The child starts writing the new AOF in a temporary file. The parent accumulates all the new changes in an in-memory buffer (but at the same time it writes the new changes in the old append-only file, so if the rewriting fails, we are safe). When the child is done rewriting the file, the parent gets a signal, and appends the in-memory buffer at the end of the file generated by the child. Profit! Now Redis atomically renames the old file into the new one, and starts appending new data into the new file. How I can switch to AOF, if I’m currently using dump.rdb snapshots?There is a different procedure to do this in Redis 2.0 and Redis 2.2, as you can guess it’s simpler in Redis 2.2 and does not require a restart at all. Redis &gt;= 2.2 Make a backup of your latest dump.rdb file. Transfer this backup into a safe place. Issue the following two commands: redis-cli config set appendonly yes redis-cli config set save “” Make sure that your database contains the same number of keys it contained. Make sure that writes are appended to the append only file correctly. The first CONFIG command enables the Append Only File. In order to do so Redis will block to generate the initial dump, then will open the file for writing, and will start appending all the next write queries. The second CONFIG command is used to turn off snapshotting persistence. This is optional, if you wish you can take both the persistence methods enabled. IMPORTANT: remember to edit your redis.conf to turn on the AOF, otherwise when you restart the server the configuration changes will be lost and the server will start again with the old configuration. Redis 2.0 Make a backup of your latest dump.rdb file. Transfer this backup into a safe place. Stop all the writes against the database! Issue a redis-cli bgrewriteaof. This will create the append only file. Stop the server when Redis finished generating the AOF dump. Edit redis.conf end enable append only file persistence. Restart the server. Make sure that your database contains the same number of keys it contained. Make sure that writes are appended to the append only file correctly. Interactions between AOF and RDB persistenceRedis &gt;= 2.4 makes sure to avoid triggering an AOF rewrite when an RDB snapshotting operation is already in progress, or allowing a BGSAVE while the AOF rewrite is in progress. This prevents two Redis background processes from doing heavy disk I/O at the same time. When snapshotting is in progress and the user explicitly requests a log rewrite operation using BGREWRITEAOF the server will reply with an OK status code telling the user the operation is scheduled, and the rewrite will start once the snapshotting is completed. In the case both AOF and RDB persistence are enabled and Redis restarts the AOF file will be used to reconstruct the original dataset since it is guaranteed to be the most complete. Backing up Redis dataBefore starting this section, make sure to read the following sentence: Make Sure to Backup Your Database. Disks break, instances in the cloud disappear, and so forth: no backups means huge risk of data disappearing into /dev/null. Redis is very data backup friendly since you can copy RDB files while the database is running: the RDB is never modified once produced, and while it gets produced it uses a temporary name and is renamed into its final destination atomically using rename(2) only when the new snapshot is complete. This means that copying the RDB file is completely safe while the server is running. This is what we suggest: Create a cron job in your server creating hourly snapshots of the RDB file in one directory, and daily snapshots in a different directory. Every time the cron script runs, make sure to call the find command to make sure too old snapshots are deleted: for instance you can take hourly snapshots for the latest 48 hours, and daily snapshots for one or two months. Make sure to name the snapshots with data and time information. At least one time every day make sure to transfer an RDB snapshot outside your data center or at least outside the physical machine running your Redis instance. Disaster recoveryDisaster recovery in the context of Redis is basically the same story as backups, plus the ability to transfer those backups in many different external data centers. This way data is secured even in the case of some catastrophic event affecting the main data center where Redis is running and producing its snapshots. Since many Redis users are in the startup scene and thus don’t have plenty of money to spend we’ll review the most interesting disaster recovery techniques that don’t have too high costs. Amazon S3 and other similar services are a good way for mounting your disaster recovery system. Simply transfer your daily or hourly RDB snapshot to S3 in an encrypted form. You can encrypt your data using gpg -c (in symmetric encryption mode). Make sure to store your password in many different safe places (for instance give a copy to the most important people of your organization). It is recommended to use multiple storage services for improved data safety. Transfer your snapshots using SCP (part of SSH) to far servers. This is a fairly simple and safe route: get a small VPS in a place that is very far from you, install ssh there, and generate an ssh client key without passphrase, then add it in the authorized_keys file of your small VPS. You are ready to transfer backups in an automated fashion. Get at least two VPS in two different providers for best results. It is important to understand that this system can easily fail if not coded in the right way. At least make absolutely sure that after the transfer is completed you are able to verify the file size (that should match the one of the file you copied) and possibly the SHA1 digest if you are using a VPS. You also need some kind of independent alert system if the transfer of fresh backups is not working for some reason.]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-introduction]]></title>
    <url>%2F2018%2F01%2F16%2Fredis-introduction%2F</url>
    <content type="text"><![CDATA[什么是Redisredis是远程的;redis是基于内存的;redis是非关系型数据库.优点：1.支持丰富的数据类型：String，List，Set，Sorted Set，Hash等2.支持两种数据持久化方式：Snapshotting（内存快照）和Append-Only file（日志追加）3.支持主从复制 Redis的应用场景 缓存 队列–使用list结构 数据存储 Redis数据类型 table th:first-of-type { width: 100px; } 数据类型 存储的值 读写能力 String 可以是字符串，整数或者浮点，统称为元素 对字符串操作，对整数类型加减 List 一个序列集合且每个节点都包好了一个元素 序列两端推入或弹出元素 Set 各个不同的元素 从集合中插入或删除元素 Hash 有key-value的散列组，其中key是字符串，value是元素 按照key进行增加删除 Sort Set 带分数的score-value有序集合，其中score是浮点，value是元素 集合插入，按照分数范围查找 String 1234567891011121314151617# key value(string/int/float)127.0.0.1:6379&gt; set string1 demoOK127.0.0.1:6379&gt; get string1&quot;demo&quot;127.0.0.1:6379&gt; set string2 4OK127.0.0.1:6379&gt; get string2&quot;4&quot; 127.0.0.1:6379&gt; incr string2 #对整型进行自增操作(integer) 5127.0.0.1:6379&gt; get string2&quot;5&quot;127.0.0.1:6379&gt; decrby string2 2 #对整型进行减法操作，将string2减去2(integer) 3127.0.0.1:6379&gt; get string2&quot;3&quot; List类型 1234567891011121314127.0.0.1:6379&gt; lpush list1 12 #lpush表示从左边push一个元素到list1中，l表示left(integer) 1127.0.0.1:6379&gt; lpush list1 13(interger) 2127.0.0.1:6379&gt; rpop list1 #rpop表示从右侧pop出一个元素，按照先入先出的原则“12”127.0.0.1:6379&gt; lpush list2 12(integer) 1127.0.0.1:6379&gt; lpush list2 13(integer) 2127.0.0.1:6379&gt; lpush list2 13 #list类型不要求集合中的元素唯一，所以可以插入相同的元素，而set类型要求集合中元素必须唯一(integer) 3127.0.0.1:6379&gt; llen list2 #llen命令列出list2中元素的个数(integer) 3 Set类型 12345678910111213141516127.0.0.1:6379&gt; sadd set1 12(integer) 1127.0.0.1:6379&gt; scard set1 #用scard查看set1中的元素个数(integer) 1127.0.0.1:6379&gt; sadd set1 13(integer) 1127.0.0.1:6379&gt; sadd set1 13(integer) 0127.0.0.1:6379&gt; scard set1(integer) 2127.0.0.1:6379&gt; sismember set1 13 #sismember 命令判断13是否在set1中(integer) 1127.0.0.1:6379&gt; srem set1 13 #srem命令将13从set1中删除(integer) 1127.0.0.1:6379&gt; sismember set1 13 (integer) 0 Hash类型 1234567891011121314151617127.0.0.1:6379&gt; hset hash1 key1 12 #hset命令设置hash1的键为key1,值为12(integer) 1127.0.0.1:6379&gt; hget hash1 key1 #hget命令获取hash1键名为key1的值&quot;12&quot;127.0.0.1:6379&gt; hset hash1 key2 13(integer) 1127.0.0.1:6379&gt; hset hash1 key3 13(integer) 1127.0.0.1:6379&gt; hlen hash1 #hlen命令获取hash1的长度(integer) 3127.0.0.1:6379&gt; hset hash1 key3 14 #直接修改hash1中key3的值(integer) 0127.0.0.1:6379&gt; hget hash1 key3&quot;14&quot;127.0.0.1:6379&gt; hmget hash1 key1 key2 #hmget 命令一次获取多个key的值1) &quot;12&quot;2) &quot;13&quot; Sort Set类型 1234567891011121314151617181920212223242526272829303132333435127.0.0.1:6379&gt; zadd zset1 10.1 val1 #zadd命令往zset1中添加一个元素，score为10.1，value为val1(integer) 1127.0.0.1:6379&gt; zadd zset1 11.2 val2(integer) 1127.0.0.1:6379&gt; zadd zset1 9.1 val3(integer) 1127.0.0.1:6379&gt; zcard zset1 #zcard 命令查看zset1中的元素值(integer) 3127.0.0.1:6379&gt; zrange zset1 0 2 withscores #zrange 命令打印出排名1) &quot;val3&quot;2) &quot;9.1&quot;3) &quot;val1&quot;4) &quot;10.1&quot;5) &quot;val2&quot;6) &quot;11.19999999&quot;127.0.0.1:6379&gt; zrange zset1 val2 #打印出val2的排名(integer) 2127.0.0.1:6379&gt; zadd zset1 12.2 val3(integer) 0127.0.0.1:6379&gt; zrange zset1 0 2 withscores1) &quot;val1&quot;2) &quot;10.1&quot;3) &quot;val2&quot;4) &quot;11.19999999&quot;5) &quot;val3&quot;6) &quot;12.19999999&quot;127.0.0.1:6379&gt; zadd zset1 12.2 val2(integer) 0127.0.0.1:6379&gt; zrange zset1 0 2 withscores1) &quot;val1&quot;2) &quot;10.1&quot;3) &quot;val2&quot;4) &quot;12.19999999&quot;5) &quot;val3&quot;6) &quot;12.19999999&quot; 常用命令键值相关命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// 1. keys *，代表取出所有的 key redis 127.0.0.1:6379&gt; keys *1) &quot;myzset2&quot;2) &quot;myzset3&quot;3) &quot;mylist&quot;4) &quot;myset2&quot;5) &quot;myset3&quot;6) &quot;myset4&quot;7) &quot;k_zs_1&quot;8) &quot;myset5&quot;9) &quot;myset6&quot;10) &quot;myset7&quot;11) &quot;myhash&quot;12) &quot;myzset&quot;13) &quot;age&quot;14) &quot;myset&quot;15) &quot;mylist5&quot;16) &quot;mylist6&quot;redis 127.0.0.1:6379&gt; keys mylist*1) &quot;mylist&quot;2) &quot;mylist5&quot;3) &quot;mylist6&quot;4) &quot;mylist7&quot;5) &quot;mylist8&quot;redis 127.0.0.1:6379&gt;// 2.exists 确认一个 key 是否存在redis 127.0.0.1:6379&gt; exists HongWan(integer) 0redis 127.0.0.1:6379&gt; exists age(integer) 1redis 127.0.0.1:6379&gt;// 3. del 删除一个 keyredis 127.0.0.1:6379&gt; del age(integer) 1redis 127.0.0.1:6379&gt; exists age(integer) 0redis 127.0.0.1:6379&gt;// 4. expire 设置一个 key 的过期时间(单位:秒)redis 127.0.0.1:6379&gt; expire addr 10(integer) 1redis 127.0.0.1:6379&gt; ttl addr(integer) 8redis 127.0.0.1:6379&gt; ttl addr(integer) 1redis 127.0.0.1:6379&gt; ttl addr(integer) -1// 们设置 addr 这个 key 的过期时间是 10 秒，然后我们不断的用 ttl 来获取这个 key 的有效时长，直至为-1 说明此值已过期// 5. move将当前数据库中的 key 转移到其它数据库中redis 127.0.0.1:6379&gt; select 0www.ChinaDBA.net 中国 DBA 超级论坛49OKredis 127.0.0.1:6379&gt; set age 30OKredis 127.0.0.1:6379&gt; get age&quot;30&quot;redis 127.0.0.1:6379&gt; move age 1(integer) 1redis 127.0.0.1:6379&gt; get age(nil)redis 127.0.0.1:6379&gt; select 1OKredis 127.0.0.1:6379[1]&gt; get age&quot;30&quot;redis 127.0.0.1:6379[1]&gt;// 先显式的选择了数据库 0，然后在这个库中设置一个 key，接下来我们将这个key 从数据库 0 移到数据库 1，之后我们确认在数据库 0 中无此 key 了, 但在数据库 1 中存在这个key，说明我们转移成功了// 6. persist 移除给定 key 的过期时间redis 127.0.0.1:6379[1]&gt; expire age 300(integer) 1redis 127.0.0.1:6379[1]&gt; ttl age(integer) 294redis 127.0.0.1:6379[1]&gt; persist age(integer) 1redis 127.0.0.1:6379[1]&gt; ttl age(integer) -1redis 127.0.0.1:6379[1]&gt;// 手动的将未到过期时间的 key，成功设置为过期// 7. randomkey 随机返回 key 空间的一个 keyredis 127.0.0.1:6379&gt; randomkey&quot;mylist7&quot;redis 127.0.0.1:6379&gt; randomkey&quot;mylist5&quot;redis 127.0.0.1:6379&gt;// 8. rename 重命名 keyredis 127.0.0.1:6379[1]&gt; keys *1) &quot;age&quot;redis 127.0.0.1:6379[1]&gt; rename age age_newOKredis 127.0.0.1:6379[1]&gt; keys *1) &quot;age_new&quot;// 9. type 返回值的类型redis 127.0.0.1:6379&gt; type addrstringredis 127.0.0.1:6379&gt; type myzset2zsetredis 127.0.0.1:6379&gt; type mylistlist 服务器相关命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 1. ping 测试连接是否存活redis 127.0.0.1:6379&gt; pingPONG// 2. echo 在命令行打印一些内容redis 127.0.0.1:6379&gt; echo Helloworld&quot;Helloworld&quot;// 3. select 选择数据库。Redis 数据库编号从 0~15，我们可以选择任意一个数据库来进行数据的存取。redis 127.0.0.1:6379&gt; select 1OKredis 127.0.0.1:6379[1]&gt; select 16(error) ERR invalid DB indexredis 127.0.0.1:6379[16]&gt;// 4. quit 退出连接redis 127.0.0.1:6379&gt; quit[root@localhost redis-2.2.12]#// 5. dbsize 返回当前数据库中 key 的数目redis 127.0.0.1:6379&gt; dbsize(integer) 18redis 127.0.0.1:6379&gt;// 6. info 获取服务器的信息和统计redis 127.0.0.1:6379&gt; info# Serverredis_version:3.2.100redis_git_sha1:00000000redis_git_dirty:0redis_build_id:dd26f1f93c5130eeredis_mode:standaloneos:Windowsarch_bits:64multiplexing_api:WinSock_IOCPprocess_id:2528..redis 127.0.0.1:6379&gt;// 7. config get 获取服务器配置信息, config get * 表示获取全部redis 127.0.0.1:6379&gt; config get dir1) &quot;dir&quot;2) &quot;/root/4setup/redis-3.2.100&quot;redis 127.0.0.1:6379&gt;// 8. flushdb 删除当前选择数据库中的所有 keyredis 127.0.0.1:6379&gt; dbsize(integer) 18redis 127.0.0.1:6379&gt; flushdbOKredis 127.0.0.1:6379&gt; dbsize(integer) 0// 9. flushall 删除所有数据库中的所有 keyredis 127.0.0.1:6379[1]&gt; dbsize(integer) 1redis 127.0.0.1:6379[1]&gt; select 0OKredis 127.0.0.1:6379&gt; flushallOKredis 127.0.0.1:6379&gt; select 1OKredis 127.0.0.1:6379[1]&gt; dbsize(integer) 0redis 127.0.0.1:6379[1]&gt;]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Werkzeug实现密码散列]]></title>
    <url>%2F2018%2F01%2F10%2F%E4%BD%BF%E7%94%A8Werkzeug%E5%AE%9E%E7%8E%B0%E5%AF%86%E7%A0%81%E6%95%A3%E5%88%97%2F</url>
    <content type="text"><![CDATA[这一功能的实现只需要两个函数，分别用在注册用户和验证用户阶段。 generate_password_hash(password, method= pbkdf2:sha1 , salt_length=8) :这个函数将原始密码作为输入，以字符串形式输出密码的散列值，输出的值可保存在用户数据库中 check_password_hash(hash, password) :这个函数的参数是从数据库中取回的密码散列值和用户输入的密码进行比对。返回值为 True 表明密码正确。 Sample：12345678910111213141516171819from werkzeug.security import generate_password_hash, check_password_hashclass User(UserMixin, db.Model): __tablename__ = &apos;users&apos; #... id = db.Column(db.Integer, primary_key=True) email = db.Column(db.String(64), unique=True, index=True) username = db.Column(db.String(64), unique=True, index=True) password_hash = db.Column(db.String(128)) @property def password(self): raise AttributeError(&apos;password is not a readable attribute&apos;) @password.setter def password(self, password): self.password_hash = generate_password_hash(password) def verify_password(self, password): return check_password_hash(self.password_hash, password)]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>Werkzeug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nfs]]></title>
    <url>%2F2018%2F01%2F06%2Fnfs%2F</url>
    <content type="text"><![CDATA[什么是NFS NFS就是Network FileSystem的缩写。它最大的功能就是可以透过网络，让不同的机器、不同的操作系统、可以彼此分享个别的档案 。 什么是RPCRPC是Remote Procedure Call的缩写。客户端就是通过远程调用RPC服务才知道该连接服务端的哪个端口号。RPC 最主要的功能就是在指定每个 NFS 功能所对应的 端口号，并且回报给客户端，让客户端可以连结到正确的端口上去。 那 RPC 又是如何知道每个 NFS 的埠口呢？这是因为当服务器在启动NFS 时会随机取用数个端口，并主动的向 RPC 注册，因此 RPC 可以知道每个端口对应的 NFS 功能，然后 RPC 又是固定使用 port 111 来监听客户端的需求并回报客户端正确的端口。NOTE：PRC要在NFS之前启动，否则NFS会无法向RPC注册。 NFS启动的RPC damons我们现在知道 NFS 服务器在启动的时候就得要向 RPC 注册，所以 NFS 服务器也被称为 RPC server 之一。 那么 NFS 服务器主要的任务是进行文件系统的分享，文件系统的分享则与权限有关。 所以 NFS 服务器启动时至少需要两个 daemons ，一个管理客户端是否能够登入的问题， 一个管理客户端能够取得的权限。如果你还想要管理 quota 的话，那么 NFS 还得要再加载其他的 RPC 程序就是了。我们以较单纯的 NFS 服务器来说： rpc.nfsd:最主要的 NFS 服务器服务提供商。这个 daemon 主要的功能就是在管理客户端是否能够使用服务器文件系统挂载信息等， 其中还包含这个登入者的 ID 的判别喔！ rpc.mountd这个 daemon 主要的功能，则是在管理 NFS 的文件系统哩！当客户端顺利的通过 rpc.nfsd 而登入服务器之后，在他可以使用 NFS 服务器提供的档案之前，还会经过档案权限 (就是那个 -rwxrwxrwx 与 owner, group 那几个权限啦) 的认证程序！他会去读 NFS 的配置文件 /etc/exports 来比对客户端的权限，当通过这一关之后客户端就可以取得使用 NFS 档案的权限啦！(注：这个也是我们用来管理 NFS 分享之目录的权限与安全设定的地方哩！) rpc.locked(非必要)这个玩意儿可以用在管理档案的锁定 (lock) 用途。为何档案需要『锁定』呢？ 因为既然分享的 NFS 档案可以让客户端使用，那么当多个客户端同时尝试写入某个档案时， 就可能对于该档案造成一些问题啦！这个 rpc.lockd 则可以用来克服这个问题。 但 rpc.lockd 必须要同时在客户端与服务器端都开启才行喔！此外， rpc.lockd 也常与 rpc.statd 同时启用。 rpc.statd(非必要)可以用来检查档案的一致性，与 rpc.lockd 有关！若发生因为客户端同时使用同一档案造成档案可能有所损毁时， rpc.statd 可以用来检测并尝试回复该档案。与 rpc.lockd 同样的，这个功能必须要在服务器端与客户端都启动才会生效。 NFS服务配置服务端配置安装必要的包使用NFS服务需要安装两个包：nfs-utils和rpcbind1yum install -y nfs-utils 使用yum安装nfs-utils时会自动安装rpcbind 配置/etc/export文件12# vim /etc/exports/tmp 192.168.100.0/24(ro) localhost(rw) *(rw,no_root_squash) 在配置文件中增加内容，每一行分为三部分： 本地要共享出去的目录 允许访问的主机 权限选项每一行最前面是要分享出来的目录，注意喔！是以目录为单位啊！ 然后这个目录可以依照不同的权限分享给不同的主机，像鸟哥上面的例子说明是： 要将 /tmp 分别分享给三个不同的主机或网域的意思。记得主机后面以小括号 () 设计权限参数， 若权限参数不止一个时，则以逗号 (,) 分开。且主机名与小括号是连在一起的喔！在这个档案内也可以利用 # 来批注呢。权限部分参数说明： rw 表示读/写 ro 表示只读 sync 表示数据同步写入内存缓冲区与磁盘中，效率较低，但可以保证数据的一致性（适合于小文件传输） async 表示数据先暂时放于内存，而非直接写入硬盘，等到必要时才写入磁盘（适合于大文件传输） no_root_squash 表示root用户对这个共享的目录拥有至高的控制权（不安全，不建议使用） root_squash 表示root用户对这个共享的目录的权限和普通用户一样。 all_squash 表示不管使用NFS的用户是谁，其身份都会被限定成一个指定的普通用户。 no_all_squash 表示所有的普通用户使用nfs都不使用权限压缩（默认设置） anonuid/anongid 要和root_squash以及all_squash选项一同使用，用于指定使用NFS的用户被限定后的uid和gid启动NFS服务在启动NFS服务之前，需要先启动rpcbind12345service rpcbind startservice nfs startORsystemctl start rpcbind.servicesystemctl stop nfs.service 关闭NFS服务12systemctl stop rpcbind.servicesystemctl stop nfs.service 客户端挂载NFS查看服务器共享的目录1showmount -e 服务器ip地址 会得到如下结果：12Export list for 192.168.0.1:/data * 在客户端挂载NFS12# mount -t nfs 服务器ip地址:服务器共享目录 挂载点mount -t nfs 192.168.0.1:/data /mnt 其中-t nfs指定挂载的类型为nfs 查看是否挂载成功命令df用于查看已挂载磁盘的总容量，使用容量，剩余容量等。1#df -h -h 表示使用合适的单位显示 解除挂载umount用于解除挂载，格式如下：1# umount 已挂载的目录 如果遇到：umount.nfs：已挂载目录:device is busy可以添加-l参数，如下：1# umount -l 已挂载目录 选项-l并不是马上umount，二是在该目录空闲后再umount，即延迟挂载。 开机自动挂载方法A：/etc/fstab里添加如下内容：12# 服务器ip地址:共享的目录 客户端挂载点 nfs defaults 1 1192.168.0.1:/data/ /mnt nfs defaults 0 0 第一个1表示备份文件系统，第二个1表示从/分区的顺序开始fsck磁盘检测，0表示不检测。方法B：将手动挂载的命令加入到/etc/rc.local中。命令exportfs命令参数： -a表示全部挂载或者卸载 -r表示重新挂载 -u表示卸载某一目录 -v表示显示共享的目录使用命令：修改配置文件/etx/exports后，使用exportfs命令挂载不需要重启NFS服务1# exportfs -arv]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask-sqlachemy]]></title>
    <url>%2F2018%2F01%2F05%2Fflask-sqlachemy%2F</url>
    <content type="text"><![CDATA[Simple Example简单的一个例子：1234567class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return &apos;&lt;User %r&gt;&apos; % self.username 使用Column定义一个字段。常用字段类型： 类型 说明 Integer 整数 String（size） 有最大长度的字符串 Text 长unicode文本 DateTime 表示datetime对象的时间和日期 Float 存储浮点值 Boolean 存储布尔值 PickleType 存储一个持久化python对象 LargeBinary 存储任意大的二进制数据 One-to-Many Relationships最常用的关系就是一对多关系。因为关系在它们建立之前就已经声明，你可以使用 字符串来参考还没有创建的类（比如如果 Person 定义了一个到 Address 的 关系，而这个关系在文件的后面才会声明）。关系用函数relationship()来表示。而外键必须用sqlalchemy.schema.ForeignKey来单独声明:12345678910class Person(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(50)) addresses = db.relationship(&apos;Address&apos;, backref=&apos;person&apos;, lazy=&apos;dynamic&apos;)class Address(db.Model): id = db.Column(db.Integer, primary_key=True) email = db.Column(db.String(50)) person_id = db.Column(db.Integer, db.ForeignKey(&apos;person.id&apos;)) db.relationship() 做了什么？这个函数返回一个可以做许多事情的属性。 在本案例中，我们让它指向 Address 类并加载那些中的多个。它如何知道这 会返回至少一个地址？因为 SQLALchemy 从你的声明中猜测了一个有用的默认值。 如果你想要一对一联系，你可以把 uselist=False 传给relationship(). So what do backref and lazy mean? backref is a simple way to also declare a new property on the Address class. You can then also use my_address.person to get to the person at that address. lazy defines when SQLAlchemy will load the data from the database: 那么 backref 和 lazy 意味着什么？ backref 是一个同样在 Address 类 上声明新属性的简单方法。你之后也可以用 my_address.person 来获取这个地址 的人。 lazy 决定了 SQLAlchemy 什么时候从数据库中加载数据: ‘select’ （默认值）意味着 SQLAlchemy 会在使用一个标准 select 语句 时一气呵成加载那些数据. ‘joined’ 让 SQLAlchemy 当父级使用 JOIN 语句是，在相同的查询中加 载关系。 ‘subquery’ 类似 ‘joined’ ，但是 SQLAlchemy 会使用子查询。 ‘dynamic’ 在你有很多条目的时侯是特别有用的。 SQLAlchemy 会返回另一个查询对象，你可以在加载这些条目时进一步提取。如果不仅想要关系下的少量条目 时，这通常是你想要的。 你如何为反向引用（backrefs）定义惰性（lazy）状态？使用backref()函数：12345class User(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(50), nullable=False) addresses = db.relationship(&apos;Address&apos;, lazy=&apos;select&apos;, backref=db.backref(&apos;person&apos;, lazy=&apos;joined&apos;)) Many-to-Many Relationships如果你想要用多对多关系，你需要定义一个用于关系的辅助表。对于这个辅助表， 强烈建议不使用模型，而是采用一个实际的表:123456789101112tags = db.Table(&apos;tags&apos;, db.Column(&apos;tag_id&apos;, db.Integer, db.ForeignKey(&apos;tag.id&apos;)), db.Column(&apos;page_id&apos;, db.Integer, db.ForeignKey(&apos;page.id&apos;)))class Page(db.Model): id = db.Column(db.Integer, primary_key=True) tags = db.relationship(&apos;Tag&apos;, secondary=tags, backref=db.backref(&apos;pages&apos;, lazy=&apos;dynamic&apos;))class Tag(db.Model): id = db.Column(db.Integer, primary_key=True) 在relationship()方法传入secondary参数，其值为关联表的表名。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>python</tag>
        <tag>sqlalchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jp-words-1]]></title>
    <url>%2F2018%2F01%2F04%2FJp-words-1%2F</url>
    <content type="text"><![CDATA[あいそう「愛想」 讨厌，招待 間柄「あいだがら」关系 敢えて「あえて」特意，并不 あなたの将来のために、敢えて忠告「ちゅうこく」します どうしても行きたいなら、私はあえて反対しない 浅ましい「あさましい」卑鄙 欺く「あざむく」 嘲笑う「あさわらう」 あせる「焦る、褪せる」着急 朝寝坊「あさねぼう」睡懒觉 後回し「あとまわし」推迟 彼が自分のことを後回しにしても、他の人を助けるような人だ。 あやふや 含糊 彼のあやふやな態度に、彼女は激怒「げきど」した。 過ち「あやまち」 过错 誰でも若い時は、過ちの一つや二つおかす。 あらっぽい「荒っぽい、粗っぽい」 粗野 彼はあらっぽい性格に見えますが、実は優しい人なんです。 あらかじめ「予め」 预先 あらかじめ必要なものをメモしていくと、無駄な買い物をしない。 ありのまま 老实，坦白 ありのままの私を認めてくれる人と結婚したいと思っている 案の定「あんのじょう」果然，果如所料 連休中の新幹線は、案の定、込んでいた。 いかにも 果然 いかにも、おっしゃる通りです。彼はいかにも優等生のダイプだ。 幾多「いくた」许多 父は、幾多の困難を乗り越えて、会社大きくしてきたそうだ 一括「いっかつ」 汇总。一包在内 時間がないので、三つの議案を一括して審議「しんぎ」する 意図「いと」 意图 著者のいとがよくわからない本だ 今更「いまさら」 事到如今 いまさら謝られでも、もう遲い。 いやいや「嫌々」 勉勉强强 内訳「うちわけ」 详细内容 進歩「しんぽ」 进步]]></content>
      <categories>
        <category>japaness</category>
      </categories>
      <tags>
        <tag>japaness</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vagrant]]></title>
    <url>%2F2018%2F01%2F04%2Fvagrant%2F</url>
    <content type="text"><![CDATA[Vagrant常用命令Vagrant Cmd： vagrant box add 添加box的操作 vagrant init 初始化box的操作 vagrant up 启动虚拟机的操作 vagrant ssh 登录虚拟机的操作 Vagrant还包括如下一些操作： vagrant box list 显示当前已经添加的box列表 $ vagrant box list base (virtualbox) vagrant box remove 删除相应的box $ vagrant box remove base virtualbox Removing box &apos;base&apos; with provider &apos;virtualbox&apos;... vagrant destroy 停止当前正在运行的虚拟机并销毁所有创建的资源 $ vagrant destroy Are you sure you want to destroy the &apos;default&apos; VM? [y/N] y [default] Destroying VM and associated drives... vagrant halt 关机 $ vagrant halt [default] Attempting graceful shutdown of VM... vagrant package 打包命令，可以把当前的运行的虚拟机环境进行打包 $ vagrant package [default] Attempting graceful shutdown of VM... [default] Clearing any previously set forwarded ports... [default] Creating temporary directory for export... [default] Exporting VM... [default] Compressing package to: /Users/astaxie/vagrant/package.box vagrant plugin 用于安装卸载插件 vagrant provision 通常情况下Box只做最基本的设置，而不是设置好所有的环境，因此Vagrant通常使用Chef或者Puppet来做进一步的环境搭建。那么Chef或者Puppet称为provisioning，而该命令就是指定开启相应的provisioning。按照Vagrant作者的说法，所谓的provisioning就是”The problem of installing software on a booted system”的意思。除了Chef和Puppet这些主流的配置管理工具之外，我们还可以使用Shell来编写安装脚本。 例如： vagrant provision --provision-with chef vagrant reload 重新启动虚拟机，主要用于重新载入配置文件 $ vagrant reload [default] Attempting graceful shutdown of VM... [default] Setting the name of the VM... [default] Clearing any previously set forwarded ports... [default] Creating shared folders metadata... [default] Clearing any previously set network interfaces... [default] Preparing network interfaces based on configuration... [default] Forwarding ports... [default] -- 22 =&gt; 2222 (adapter 1) [default] Booting VM... [default] Waiting for VM to boot. This can take a few minutes. [default] VM booted and ready for use! [default] Setting hostname... [default] Mounting shared folders... [default] -- /vagrant vagrant resume 恢复前面被挂起的状态 $vagrant resume [default] Resuming suspended VM... [default] Booting VM... [default] Waiting for VM to boot. This can take a few minutes. [default] VM booted and ready for use! vagrant ssh-config 输出用于ssh连接的一些信息 $vagrant ssh-config Host default HostName 127.0.0.1 User vagrant Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile &quot;/Users/astaxie/.vagrant.d/insecure_private_key&quot; IdentitiesOnly yes LogLevel FATAL vagrant status 获取当前虚拟机的状态 $vagrant status Current machine states: default running (virtualbox) The VM is running. To stop this VM, you can run `vagrant halt` to shut it down forcefully, or you can run `vagrant suspend` to simply suspend the virtual machine. In either case, to restart it again, simply run `vagrant up`. vagrant suspend 挂起当前的虚拟机 $ vagrant suspend [default] Saving VM state and suspending execution... 模拟打造多机器的分布式系统前面这些单主机单虚拟机主要是用来自己做开发机，从这部分开始的内容主要将向大家介绍如何在单机上通过虚拟机来打造分布式造集群系统。这种多机器模式特别适合以下几种人： 快速建立产品网络的多机器环境，例如web服务器、db服务器 建立一个分布式系统，学习他们是如何交互的 测试API和其他组件的通信 容灾模拟，网络断网、机器死机、连接超时等情况 Vagrant支持单机模拟多台机器，而且支持一个配置文件Vagrntfile就可以跑分布式系统。 现在我们来建立多台VM跑起來，並且让他们之间能够相通信，假设一台是应用服务器、一台是DB服务器，那么这个结构在Vagrant中非常简单，其实和单台的配置差不多，你只需要通过config.vm.define来定义不同的角色就可以了，现在我们打开配置文件进行如下设置： Vagrant.configure(&quot;2&quot;) do |config| config.vm.define :web do |web| web.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;web&quot;, &quot;--memory&quot;, &quot;512&quot;] end web.vm.box = &quot;base&quot; web.vm.hostname = &quot;web&quot; web.vm.network :private_network, ip: &quot;11.11.1.1&quot; end config.vm.define :db do |db| db.vm.provider &quot;virtualbox&quot; do |v| v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;db&quot;, &quot;--memory&quot;, &quot;512&quot;] end db.vm.box = &quot;base&quot; db.vm.hostname = &quot;db&quot; db.vm.network :private_network, ip: &quot;11.11.1.2&quot; end end 这里的设置和前面我们单机设置配置类似，只是我们使用了:web以及:db分別做了两个VM的设置，并且给每个VM设置了不同的hostname和IP，设置好之后再使用vagrant up将虚拟机跑起来： $ vagrant up Bringing machine &apos;web&apos; up with &apos;virtualbox&apos; provider... Bringing machine &apos;db&apos; up with &apos;virtualbox&apos; provider... [web] Setting the name of the VM... [web] Clearing any previously set forwarded ports... [web] Creating shared folders metadata... [web] Clearing any previously set network interfaces... [web] Preparing network interfaces based on configuration... [web] Forwarding ports... [web] -- 22 =&gt; 2222 (adapter 1) [web] Running any VM customizations... [web] Booting VM... [web] Waiting for VM to boot. This can take a few minutes. [web] VM booted and ready for use! [web] Setting hostname... [web] Configuring and enabling network interfaces... [web] Mounting shared folders... [web] -- /vagrant [db] Setting the name of the VM... [db] Clearing any previously set forwarded ports... [db] Fixed port collision for 22 =&gt; 2222. Now on port 2200. [db] Creating shared folders metadata... [db] Clearing any previously set network interfaces... [db] Preparing network interfaces based on configuration... [db] Forwarding ports... [db] -- 22 =&gt; 2200 (adapter 1) [db] Running any VM customizations... [db] Booting VM... [db] Waiting for VM to boot. This can take a few minutes. [db] VM booted and ready for use! [db] Setting hostname... [db] Configuring and enabling network interfaces... [db] Mounting shared folders... [db] -- /vagrant 看到上面的信息输出后，我们就可以通过vagrant ssh登录虚拟机了，但是这次和上次使用的不一样了，这次我们需要指定相应的角色，用来告诉ssh你期望连接的是哪一台： $ vagrant ssh web vagrant@web:~$ $ vagrant ssh db vagrant@db:~$ 是不是很酷！现在接下来我们再来验证一下虚拟机之间的通信，让我们先使用ssh登录web虚拟机，然后在web虚拟机上使用ssh登录db虚拟机(默认密码是vagrant)： $ vagrant ssh web Linux web 2.6.32-38-server #83-Ubuntu SMP Wed Jan 4 11:26:59 UTC 2012 x86_64 GNU/Linux Ubuntu 10.04.4 LTS Welcome to the Ubuntu Server! * Documentation: http://www.ubuntu.com/server/doc New release &apos;precise&apos; available. Run &apos;do-release-upgrade&apos; to upgrade to it. Welcome to your Vagrant-built virtual machine. Last login: Thu Aug 8 18:55:44 2013 from 10.0.2.2 vagrant@web:~$ ssh 11.11.1.2 The authenticity of host &apos;11.11.1.2 (11.11.1.2)&apos; can&apos;t be established. RSA key fingerprint is e7:8f:07:57:69:08:6e:fa:82:bc:1c:f6:53:3f:12:9e. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;11.11.1.2&apos; (RSA) to the list of known hosts. vagrant@11.11.1.2&apos;s password: Linux db 2.6.32-38-server #83-Ubuntu SMP Wed Jan 4 11:26:59 UTC 2012 x86_64 GNU/Linux Ubuntu 10.04.4 LTS Welcome to the Ubuntu Server! * Documentation: http://www.ubuntu.com/server/doc New release &apos;precise&apos; available. Run &apos;do-release-upgrade&apos; to upgrade to it. Welcome to your Vagrant-built virtual machine. Last login: Thu Aug 8 18:58:50 2013 from 10.0.2.2 vagrant@db:~$ 通过上面的信息我们可以看到虚拟机之间通信是畅通的，所以现在开始你伟大的架构设计吧，你想设计怎么样的架构都可以，唯一限制你的就是你主机的硬件配置了。]]></content>
      <categories>
        <category>vagrant</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo cmd]]></title>
    <url>%2F2018%2F01%2F03%2Fhexo-cmd%2F</url>
    <content type="text"><![CDATA[Hexo Cmd hexo help # 查看帮助hexo version #查看Hexo的版本hexo algolia # 更新search庫hexo new “postName” #新建文章hexo new post “title” # 生成新文章：\source_posts\title.md，可省略posthexo new page “pageName” #新建页面hexo clean #清除部署緩存hexo n == hexo new #新建文章hexo g == hexo generate #生成静态页面至public目录hexo s == hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo d == hexo deploy #将.deploy目录部署到GitHubhexo d -g #生成加部署hexo s -g #生成加预览 Clean&amp;Updatehexo cl hexo d -g]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask definitions]]></title>
    <url>%2F2018%2F01%2F03%2Fflask-definitions%2F</url>
    <content type="text"><![CDATA[Flask’s some definitions WSGI: Web服务器网关接口，是一种Web服务使用的协议 路由: 处理URL和函数之间关系的程序称为”路由” 视图函数: 类似于index()这样的，被app.route装饰器注册为路由的函数，或者通过app.add_url_rule()添加路由映射关系的函数，被称为视图函数。 app.route(): 路由装饰器，可以带参数，参数可以指定数据类型：int/float/path。path类似于字符串，但不将反斜线/当做分隔符。 Flask上下文全局变量 current_app: 程序上下文，当前激活程序的程序实例，所有线程公用一个该实例。 g: 程序上下文，处理请求时用作临时存储的对象，每次请求都会重设这个变量 request: 请求上下文，请求对象，封装了客户端发出的 HTTP 请求中的内容，不同线程之间互不干扰 session: 请求上下问，用户会话，用于存储请求之间需要“记住”的值的词典。 Flask支持的4种钩子函数 before_first_request: 注册一个函数，在处理第一个请求之前运行。 before_request: 注册一个函数，在每次请求之前运行。 after_request: 注册一个函数，如果没有未处理的异常抛出，在每次请求之后运行。 teardown_request:注册一个函数，即使有未处理的异常抛出，也在每次请求之后运行。 Jinja2模板使用渲染模板: render_template(“user.html”, name=name)控制结构：1234567891011&#123;% if user %&#125; Hello, &#123;&#123; user &#125;&#125;! &#123;% else %&#125; Hello, Stranger! &#123;% endif %&#125; &lt;ul&gt; &#123;% for comment in comments %&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; 宏-类似于函数:123456789 &#123;% macro render_comment(comment) %&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt; &#123;% endmacro %&#125; &lt;ul&gt; &#123;% for comment in comments %&#125; &#123;&#123; render_comment(comment) &#125;&#125; &#123;% endfor %&#125; &lt;/ul&gt; Jinja2变量过滤器 # safe: 渲染值时不转义# capitalize: 把值的首字母转换成大写，其他字母转换成小写# lower: 把值转换成小写形式# upper: 把值转换成大写形式# title: 把值中每个单词的首字母都转换成大写# trim: 把值的首尾空格去掉# striptags: 渲染之前把值中所有的 HTML 标签都删掉 WTForms支持的字段注意添加app.config[‘SECRET_KEY’] = ‘hard to guess string’ StringField 文本字段 TextAreaField 多行文本字段 PasswordField 密码文本字段 HiddenField 隐藏文本字段 DateField 值为datatime.data格式的文本字段 DateTimeField 值为datatime.datatime格式的文本字段 DecimalField 值为decimal.Decimal格式的文本字段 IntegerField 值为整数的文本字段 FloatField 值为浮点数的文本字段 BooleanField 值为True或False的复选框 RadioField 一组单选框 SelectField 值唯一的下拉列表 SelectMultipleField 可选多个值得下拉列表 FileField 文件上传字段 SubmitField 表单提交按钮 FormField 把表单作为字段嵌入另一个表单 FieldList 一组指定类型的字段 常见返回码200 OK - [GET]：服务器成功返回用户请求的数据201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功202 Accepted - []：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作401 Unauthorized - []：表示用户没有权限（令牌、用户名、密码错误）403 Forbidden - [] 表示用户得到授权（与401错误相对），但是访问是被禁止的404 NOT FOUND - []：用户发出的请求针对的是不存在的记录，服务器没有进行操作406 Not Acceptable - [GET]：用户请求的格式不可得410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed-grep]]></title>
    <url>%2F2018%2F01%2F01%2Fsed-grep%2F</url>
    <content type="text"><![CDATA[sedsed对文本的处理非常的强大，并且sed非常小，操作跟awk类似。sed是按顺序逐行进行读取文件。然后，它执行的是该行指定的所有操作，并在完成请求的修改之后的内容显示出来，也可以将其存放到文件之中。 sed [-nefri] [动作] -n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e ：直接在命令列模式上进行 sed 的动作编辑； -f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作； -r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法) -i ：直接修改读取的文件内容，而不是输出到终端。 –follow-symlinks 直接修改文件时跟随软链接 动作说明 ： [n1[,n2]]fuctionn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』 funtion： a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ grep命令格式： grep [option] pattern file 参数 -i：忽略大小写 -c：打印匹配的行数 -v：查找不包含匹配项的行 -n：打印包含匹配项的行和行标 正则表达式 ^ #锚定行的开始 如：’^grep’匹配所有以grep开头的行。$ #锚定行的结束 如：’grep$’匹配所有以grep结尾的行。. #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。* #匹配零个或多个先前字符 如：’grep’匹配所有一个或多个空格后紧跟grep的行。. #一起用代表任意字符。[] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。[^] #匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。(..) #标记匹配字符，如’(love)‘，love被标记为1。\&lt; #锚定单词的开始，如:’\‘匹配包含以grep结尾的单词的行。x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。\W #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>sed</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk]]></title>
    <url>%2F2017%2F12%2F29%2Fawk%2F</url>
    <content type="text"><![CDATA[awkawk是一种处理文本的语言，是一个强大的文本分析工具，awk是以列为划分记数的，$0表示所有列，$1表示第一列，$2表示第二列。 awk常用参数 -F指定输入文件分隔符，如-F： -v 赋值一个用户定义变量，如-va=1 -f 从脚本文件中读取awk命令 多个分隔符awk -F &#39;[-|]&#39; &#39;{print $3}&#39; data上面这个例子是以-和|为分隔符进行分割。 设置变量设置awk自定义变量，使用参数-vcat data.txt | awk -v a=9 &#39;{print $1,$1+a}&#39;如上，设置了变量a的值，在输出的时候添加一个$1+a的值。如果在脚本中使用的时候，前面已经定义了一个$2的变量，并且你就是像要使用这个变量的话，就要使用以下的形式：awk &#39;{print $$2}&#39; 逻辑判断cat data.txt | awk &#39;$1==&quot;reworth&quot; {print}输出第一列为reworth的所有行。cat data.txt | awk &#39;$1!=&quot;reworth&quot; {print}&#39;输出第一列不是reworth的所有行。 正则匹配cat data.txt | awk &#39;$2 ~ /reworth.*/ {print}&#39;匹配第二列中以reworth开头的所有行。对某列进行匹配时需要在列之后加个～表示进行匹配。cat data.txt | awk &#39;/reworth.*/ {print}&#39;匹配以reworth开头的所有行。匹配取反 !~cat data.txt | awk &#39;$2 !~ /reworth/ {print}&#39;匹配第二列不是reworth的所有行。 内置变量 FILENAME : 当前输入文件名称 NF : 当前输入行的字段编号 OFS : 输出字段分隔符 NR : 当前输入行编号(是指输入行 1，2，3……等) FS : 输入字段分隔符 ORS : 输出记录分隔符 RS : 输入记录分隔符 内置函数substr字符串截取cat data.txt | awk &#39;{print substr($1,1,4)}&#39;截取第一列中的第一个到第四个字符。split 切片cat data.txt |awk &#39;{split($1,a,&quot;,&quot;);print a[1],a[2],a[3]}&#39;以逗号分隔第一列，并输出分隔后的数据。gsub 替换cat data.txt | awk &#39;&#39;{gsub(&quot;abc&quot;,&quot;asd&quot;,$2);print}将第二列中的abc替换成asd 统计 grep ‘tower_activity_op’ /data/s*/log/test.log | awk -F’[=,]’ ‘{a[$2] -= $12; b[$2] -= $14; item_count[$2]++;} END{for(i in a) {serverid=i; cmd=”/usr/bin/mysql -u root -h \”192.168.0.1\” -p1234 -D stat -e \”replace into test(date,uid,cash,coins,number,code,serverid) values(\047’$Date’\047,”i”,”a[i]”,”b[i]”,”item_count[i]”,\047测试\047,”serverid”)\””; system(cmd);}}’]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[volume-container]]></title>
    <url>%2F2017%2F12%2F28%2Fvolume-container%2F</url>
    <content type="text"><![CDATA[volume container是专门为其他容器提供volume的容器。它提供的卷可以是bind mount,也可以是docker managed volume.下面我创建一个volume container:1234docker create --name vc_data \ -v ~/htdocs:/usr/local/apache2/htdocs \ -v ~/other/useful/tools \ busybox 我们将容器命名为vc_data.注意这里执行的是docker create命令，这是因为volume container的作用是只提供数据，它本身不需要处于运行状态。容器中mount了两个volume: bind mount,存放web server的静态文件。 docker managed volume,存放一些实用的工具通过docker inspect可以查看这两个volume]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>volume</tag>
      </tags>
  </entry>
</search>
